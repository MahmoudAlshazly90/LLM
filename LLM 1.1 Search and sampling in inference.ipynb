{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "483d3191",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import pipeline \n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95af84bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset xsum (C:/Users/Mahmoud Alshazly/.cache/huggingface/datasets/xsum/default/1.2.0/082863bf4754ee058a5b6f6525d0cb2b18eadb62c7b370b095d1364050a52b71)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79ea788aff6442db9064866296425e3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'the full cost of damage in Newton Stewart is still being assessed . many roads in peeblesshire remain badly affected by standing water . a flood alert remains in place across the'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer = pipeline(\n",
    "    task=\"summarization\",\n",
    "    model=\"t5-small\",\n",
    "    min_length=20,\n",
    "    max_length=40,\n",
    "    truncation=True,\n",
    "    #model_kwargs={\"cache_dir\": DA.paths.datasets},\n",
    ")\n",
    "xsum_dataset = load_dataset(\n",
    "    \"xsum\",\n",
    "    version=\"1.2.0\"\n",
    ") \n",
    "xsum_sample = xsum_dataset[\"train\"].select(range(10))\n",
    "\n",
    "summarizer(xsum_sample[\"document\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6da7e4b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'the full cost of damage in Newton Stewart is still being assessed . many roads in peeblesshire remain badly affected by standing water . a flood alert remains in place across the'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# We can instead do a beam search by specifying num_beams.\n",
    "# This takes longer to run, but it might find a better (more likely) sequence of text.\n",
    "summarizer(xsum_sample[\"document\"][0], num_beams=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b656ead1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'the full cost of damage in Newton Stewart is still being assessed . many roads in peeblesshire remain badly affected by standing water . a flood alert remains in place across the'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Alternatively, we could use sampling.\n",
    "summarizer(xsum_sample[\"document\"][0], do_sample=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a013f9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'the full cost of damage in Newton Stewart is still being assessed . many roads in peeblesshire remain badly affected by standing water . a flood alert remains in place across the'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# We can modify sampling to be more greedy by limiting sampling to the top_k or top_p most likely next tokens.\n",
    "summarizer(xsum_sample[\"document\"][0], do_sample=True, top_k=10, top_p=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "724a6a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5a30eb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>summarize: The full cost of damage in Newton S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>summarize: A fire alarm went off at the Holida...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>summarize: Ferrari appeared in a position to c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>summarize: John Edward Bates, formerly of Spal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>summarize: Patients and staff were evacuated f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>summarize: Simone Favaro got the crucial try w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>summarize: Veronica Vanessa Chango-Alverez, 31...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>summarize: Belgian cyclist Demoitie died after...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>summarize: Gundogan, 26, told BBC Sport he \"ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>summarize: The crash happened about 07:20 GMT ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             prompts\n",
       "0  summarize: The full cost of damage in Newton S...\n",
       "1  summarize: A fire alarm went off at the Holida...\n",
       "2  summarize: Ferrari appeared in a position to c...\n",
       "3  summarize: John Edward Bates, formerly of Spal...\n",
       "4  summarize: Patients and staff were evacuated f...\n",
       "5  summarize: Simone Favaro got the crucial try w...\n",
       "6  summarize: Veronica Vanessa Chango-Alverez, 31...\n",
       "7  summarize: Belgian cyclist Demoitie died after...\n",
       "8  summarize: Gundogan, 26, told BBC Sport he \"ca...\n",
       "9  summarize: The crash happened about 07:20 GMT ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For summarization, T5-small expects a prefix \"summarize: \", so we prepend that to each article as a prompt.\n",
    "articles = list(map(lambda article: \"summarize: \" + article, xsum_sample[\"document\"]))\n",
    "display(pd.DataFrame(articles, columns=[\"prompts\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e199a8c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids:\n",
      "tensor([[21603,    10,    37,  ...,     0,     0,     0],\n",
      "        [21603,    10,    71,  ...,     0,     0,     0],\n",
      "        [21603,    10, 21945,  ..., 18002,    21,     1],\n",
      "        ...,\n",
      "        [21603,    10, 21768,  ...,     0,     0,     0],\n",
      "        [21603,    10,  9982,  ...,     0,     0,     0],\n",
      "        [21603,    10,    37,  ...,     0,     0,     0]])\n",
      "attention_mask:\n",
      "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n",
      "tensor([[    0,     8,   423,   583,    13,  1783,    16, 20126, 16496,    19,\n",
      "           341,   271, 14841,     3,     5,   186,  7540,    16,   158,    15,\n",
      "          2296,     7,  5718,  2367, 14621,  4161,    57,  4125,   387,     3,\n",
      "             5,     3,     9,  8347,  5685,  3048,    16,   286,   640,     8],\n",
      "        [    0,  1472,  6196,   877,   326,    44,     8,  9108,    86,    29,\n",
      "            16,  6000,  1887,    30,  1856,     3,     5,  2554,   130,  1380,\n",
      "            12,  1175,     8,  1595,     3,     5,    80,    13,     8,   192,\n",
      "         14264,    19,    45, 13692,    63,     6,     8,   119,    45, 20576],\n",
      "        [    0,     3,   849,  2239,     7,   163, 14014,     3,    60,  8234,\n",
      "           232,   227,     3, 19585,   643,   845,   150,  8033,    47,   787,\n",
      "            30,   213,     3,    88,   225,  2447,     3,     5,     3,   849,\n",
      "          2239,     7,   497,     3,    31,    29,    32,   964,  8033,    47],\n",
      "        [    0,     8,     3,  3708,    18,  1201,    18,  1490,    19, 11970,\n",
      "            13,     3, 25345,     8, 10883,  2319,   344,  1332, 16583,    11,\n",
      "          1797,  9975,     3,     5,     3,    88,   177,   725,    66,     8,\n",
      "          3991,     6,   379,   192, 12052,    13,    16,   221,    75,  4392],\n",
      "        [    0,     3,     9,   388,  4281,  1058,    44,     8,  5998, 16026,\n",
      "            12,  4279,  2448,    11,   717,     3,     5,     3,     9,  1021,\n",
      "          2095,  5502,    47, 29766,    26,    45,     8,  2833,     3,     5,\n",
      "             8,  5415,   639, 18905,  7012,    16, 20958,   826,   633,  6032],\n",
      "        [    0,     3, 26705,  4463,     7,   989,  1891,     3,     9,  5695,\n",
      "            12,   579,  1840,     3,  3108,  2067,  1824,   400,  1823,    23,\n",
      "            63,  2551,  1967,    32,     3,     5,     8, 14580,     7,  1891,\n",
      "           166,  3511,    13,     8,   774,    12,     3,  3108,     3,     9],\n",
      "        [    0,     3, 31873, 30003, 24187,    32,    18,   188,    40,   624,\n",
      "           457,     6, 12074,    47,  4792,    11,   430,   388,  7532,    16,\n",
      "             8,  8420,     3,     5,  2095,   241,    12,  8320, 18050,  8688,\n",
      "             6, 14141,   113,    65,  2416,    12,     8,  9835,     3,     5],\n",
      "        [    0,     8,   944,    18,  1201,    18,  1490,    47,  1560,    57,\n",
      "             3,     9,  2340, 15214,   383,     8, 21689,    18,  1326,  4911,\n",
      "           397,    51,  1964,     3,     5,     8,  1964,  2804,   190,  8390,\n",
      "          2515,   663,     3,     5,     8,  2600,    31,     7,     3, 19585],\n",
      "        [    0,  4740, 10169,   152,   845,     3,    88,    54,   217,     8,\n",
      "          8619,   689,   227,     3, 30846,     3, 22052,   342,  6476, 27588,\n",
      "             7,    16,  1882,     3,     5,     8, 13692,  4785,     8,  1412,\n",
      "           296,  4119,   227,   223,  3730,    24,  2697,   376,    91,    21],\n",
      "        [    0,     8,  8420,  2817,    81, 10668,    10,  1755, 22866,    44,\n",
      "             8, 23704,    13,     8,    71, 22367,    11, 24583,  2409,    16,\n",
      "            90,  9031,    18,   106,    18,   134,    15,     9,     6, 25223,\n",
      "             3,     5,     8,   388,     6,  9742,    16,   112,   460,     7]])\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the input\n",
    "inputs = tokenizer(\n",
    "    articles, max_length=1024, return_tensors=\"pt\", padding=True, truncation=True\n",
    ")\n",
    "print(\"input_ids:\")\n",
    "print(inputs[\"input_ids\"])\n",
    "print(\"attention_mask:\")\n",
    "print(inputs[\"attention_mask\"])\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Generate summaries\n",
    "summary_ids = model.generate(\n",
    "    inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    num_beams=2,\n",
    "    min_length=0,\n",
    "    max_length=40,\n",
    ")\n",
    "print(summary_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee86483f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>decoded_summaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the full cost of damage in Newton Stewart is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fire alarm went off at the Holiday Inn in Hope...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stewards only handed reprimand after governing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the 67-year-old is accused of committing the o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a man receiving treatment at the clinic threat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Gregor Townsend gave a debut to powerhouse win...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Veronica Vanessa Chango-Alverez, 31, was kille...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>the 25-year-old was hit by a motorbike during ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gundogan says he can see the finishing line af...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>the crash happened about 07:20 GMT at the junc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   decoded_summaries\n",
       "0  the full cost of damage in Newton Stewart is s...\n",
       "1  fire alarm went off at the Holiday Inn in Hope...\n",
       "2  stewards only handed reprimand after governing...\n",
       "3  the 67-year-old is accused of committing the o...\n",
       "4  a man receiving treatment at the clinic threat...\n",
       "5  Gregor Townsend gave a debut to powerhouse win...\n",
       "6  Veronica Vanessa Chango-Alverez, 31, was kille...\n",
       "7  the 25-year-old was hit by a motorbike during ...\n",
       "8  gundogan says he can see the finishing line af...\n",
       "9  the crash happened about 07:20 GMT at the junc..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "decoded_summaries = tokenizer.batch_decode(summary_ids, skip_special_tokens=True)\n",
    "display(pd.DataFrame(decoded_summaries, columns=[\"decoded_summaries\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42dbb0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=True`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\n",
    "    \"t5-small\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a757f24e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>decoded_summaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the full cost of damage in Newton Stewart is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fire alarm went off at the Holiday Inn in Hope...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stewards only handed reprimand after governing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the 67-year-old is accused of committing the o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a man receiving treatment at the clinic threat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Gregor Townsend gave a debut to powerhouse win...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Veronica Vanessa Chango-Alverez, 31, was kille...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>the 25-year-old was hit by a motorbike during ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gundogan says he can see the finishing line af...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>the crash happened about 07:20 GMT at the junc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   decoded_summaries\n",
       "0  the full cost of damage in Newton Stewart is s...\n",
       "1  fire alarm went off at the Holiday Inn in Hope...\n",
       "2  stewards only handed reprimand after governing...\n",
       "3  the 67-year-old is accused of committing the o...\n",
       "4  a man receiving treatment at the clinic threat...\n",
       "5  Gregor Townsend gave a debut to powerhouse win...\n",
       "6  Veronica Vanessa Chango-Alverez, 31, was kille...\n",
       "7  the 25-year-old was hit by a motorbike during ...\n",
       "8  gundogan says he can see the finishing line af...\n",
       "9  the crash happened about 07:20 GMT at the junc..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "    articles, max_length=1024, return_tensors=\"pt\", padding=True, truncation=True\n",
    ")\n",
    "summary_ids = model.generate(\n",
    "    inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    num_beams=2,\n",
    "    min_length=0,\n",
    "    max_length=40,\n",
    ")\n",
    "decoded_summaries = tokenizer.batch_decode(summary_ids, skip_special_tokens=True)\n",
    "\n",
    "display(pd.DataFrame(decoded_summaries, columns=[\"decoded_summaries\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1172b9f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
